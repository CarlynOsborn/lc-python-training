{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It Together\n",
    "\n",
    "\n",
    "* The following example takes all of the different things we have learned and combines them together into a program\n",
    "* This program will processes [QuadraSTAR](https://lucidea.com/cuadrastar/) data files and converts the *narrow* data format into a *wide* data format (see [wikipedia for more discussion](https://en.wikipedia.org/wiki/Wide_and_narrow_data). \n",
    "* QuadraSTAR records looks like this:\n",
    "\n",
    "```\n",
    "--REC--4\n",
    "IU     10/09/2003\n",
    "NAME   Egger, J.C. Jr\n",
    "REFNO  0890-0-01\n",
    "SERV   USAF\n",
    "TYPE   Message\n",
    "DOR    94 08 19\n",
    "DOI    67 11 03\n",
    "IIRN   191300zAUG94\n",
    "ORG    SECDEF\n",
    "SUBJ   Public Affairs-POW/MIA-Press Guidance for identification of remains-Cambodia and Vietnam-August 94.\n",
    "CAT    Casualty files\n",
    "PAGE   11-15\n",
    "REEL   512\n",
    "REELC  1\n",
    "FOLD   115-8\n",
    "CNTRY  Cambodia\n",
    "CNTRY  North Vietnam, pre-1975\n",
    "KYWDS  Remains\n",
    "KYWDS  Aircraft downed\n",
    "KYWDS  Quang Tri Province\n",
    "RSCHR  jego\n",
    "COMM   The remains of three American servicemen previously unaccounted for in Indochina, have been identified.\n",
    "SUBJ   Public Affairs-POW/MIA-Press Guidance for identification of remains-Cambodia and Vietnam-August 94.\n",
    "CAT    Casualty files\n",
    "PAGE   11-15\n",
    "REEL   512\n",
    "REELC  1\n",
    "FOLD   115-8\n",
    "CNTRY  Cambodia\n",
    "CNTRY  North Vietnam, pre-1975\n",
    "KYWDS  Remains\n",
    "KYWDS  Aircraft downed\n",
    "KYWDS  Quang Tri Province\n",
    "RSCHR  jego\n",
    "COMM   The remains of three American servicemen previously unaccounted for in Indochina, have been identified.\n",
    "```\n",
    "\n",
    "* Each record is a series of attribute value pairs split over a series of lines separated by `--REC-<num>`\n",
    "* The QuadraSTAR data is very inconvenient because it doesn't include the record ID in each line\n",
    "* This means we have to keep track of records as we parse through the file\n",
    "* We want it to end up as a CSV file with one record per row and all the attributes as columns\n",
    "\n",
    "```\n",
    "PWMIA_4,PWMIA,4,,Casualty files,\"Cambodia; North Vietnam, pre-1975; Cambodia; North Vietnam, pre-1975\",\"The remains of three American servicemen previously unaccounted for in Indochina, have been identified.\",,67 11 03,94 08 19,,115-8,191300zAUG94,,10/09/2003,Remains; Aircraft downed; Quang Tri Province; Remains; Aircraft downed; Quang Tri Province,\"Egger, J.C. Jr\",,SECDEF,11-15,,,,512,1,0890-0-01,,jego,,USAF,,Public Affairs-POW/MIA-Press Guidance for identification of remains-Cambodia and Vietnam-August 94.,,Message,\n",
    "```\n",
    "* We will have to make two passes through the data because we need to know all the headers in advance\n",
    "* The hard part, beyond knowing the Python commands, is thinking about how to assemble the computational logic to perform the processing\n",
    "    * This *computational thinking* takes time and practice to learn\n",
    "    * There are also many ways to compose this program, smarter programmers might have more clever ways of performing this data processing\n",
    "\n",
    "\n",
    "## Workflow\n",
    "* Get a list of a QuadraSTAR files in the data directory\n",
    "* Parse the files and generate a list of headers\n",
    "    * Save that list to a file\n",
    "* Create a CSV file for saving the transformed records\n",
    "* Loop through every line of every data files\n",
    "    * Parse the line into header and value\n",
    "    * If the line is a REC start building a dictionary for the row\n",
    "    * When the record is complete write a row to the CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "\n",
    "* We are only going to use two Python libraries in this program\n",
    "* `csv` to help writing the CSV file (much better than doing it manually)\n",
    "* `pathlib` to help locating the source data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Variables\n",
    "\n",
    "* To make this program easy to change we will put some filepaths up top\n",
    "* This makes the script a bit more flexible if the files are in different directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('clean_star/PWMIA.txt'), PosixPath('clean_star/PWMIA03.txt'), PosixPath('clean_star/PWMIA04.txt'), PosixPath('clean_star/PWMIA05.txt')]\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"clean_star/\"\n",
    "header_file_name = \"headers.txt\"\n",
    "data_file_name = \"data.csv\"\n",
    "\n",
    "# Generate a list of data files using the pathlibe function\n",
    "# Explicitly make it a list rather than a generator\n",
    "file_list = list(Path(data_folder).glob(\"*.txt\"))\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Headers\n",
    "\n",
    "* Before we can start writing the CSV file we need to generate a list of the headers\n",
    "* This involves quickly reading through the files and creating a list of \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a little function to use in a list comprehension\n",
    "# for processing lines\n",
    "def process_line(line):\n",
    "    # Quadraster is a fixed width format, the first 6 characters are the header\n",
    "    # We can slice the header from the line\n",
    "    # Also replace dashes with nothing and strip extra whitespace\n",
    "    header = line[0:6].replace(\"-\", \"\").strip()\n",
    "    return header\n",
    "\n",
    "# Loop over the list of files\n",
    "for file in file_list:\n",
    "    # Open the file in read-only mode\n",
    "    with open(file, \"r\") as f:\n",
    "        # Use a set comprehension to make a list of unique header values\n",
    "        fields = {process_line(line) for line in f}\n",
    "\n",
    "# Display the identified fields\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code has a BUG!\n",
    "* It is a *semantic* bug in the logic of the program\n",
    "    * Meaning it runs fine but doesn't produce expected behavior\n",
    "* Can you figure out the bug?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CAPT',\n",
       " 'CAT',\n",
       " 'CNTRY',\n",
       " 'COMM',\n",
       " 'COND',\n",
       " 'DOI',\n",
       " 'DOR',\n",
       " 'EDIT',\n",
       " 'FOLD',\n",
       " 'IIRN',\n",
       " 'IMAGE',\n",
       " 'IU',\n",
       " 'KYWDS',\n",
       " 'NAME',\n",
       " 'NU',\n",
       " 'ORG',\n",
       " 'PAGE',\n",
       " 'PDS',\n",
       " 'PHYS',\n",
       " 'PWC',\n",
       " 'REC',\n",
       " 'REEL',\n",
       " 'REELC',\n",
       " 'REFNO',\n",
       " 'RPTNO',\n",
       " 'RSCHR',\n",
       " 'SEC',\n",
       " 'SERV',\n",
       " 'SRCNO',\n",
       " 'SUBJ',\n",
       " 'TTL',\n",
       " 'TYPE',\n",
       " 'VOL'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fields = set()\n",
    "\n",
    "# Create a little function to use in a list comprehension\n",
    "# for processing lines\n",
    "def process_line(line):\n",
    "    # Quadraster is a fixed width format, the first 6 characters are the header\n",
    "    # We can slice the header from the line\n",
    "    # Also replace dashes with nothing and strip extra whitespace\n",
    "    header = line[0:6].replace(\"-\", \"\").strip()\n",
    "    return header\n",
    "\n",
    "# Loop over the list of files\n",
    "for file in file_list:\n",
    "    # Open the file in read-only mode\n",
    "    with open(file, \"r\") as f:\n",
    "        # Use a set comprehension to make a list of unique header values\n",
    "        fields = {process_line(line) for line in f}\n",
    "        all_fields.update(fields)\n",
    "\n",
    "# Display the identified fields\n",
    "all_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote headers.txt\n"
     ]
    }
   ],
   "source": [
    "# Write the fields to disk\n",
    "with open(header_file_name, \"w\") as f:\n",
    "    # Loop over each header and write each on a new line\n",
    "    for field in sorted(all_fields):\n",
    "        f.write(field + \"\\n\")\n",
    "print(\"Wrote {}\".format(header_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Headers\n",
    "\n",
    "* Now that we have saved the headers to disk we can load this file without having to parse all of the files ever again!\n",
    "* If there are a lot of files this is a huge time saver\n",
    "* This cell opens the header file and creates a list of the column headers for the CSV file\n",
    "* It also does some re-ordering to put identifying information in the leftmost columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(header_file_name, \"r\") as f:\n",
    "    header_fields = [line.strip() for line in f]\n",
    "    \n",
    "    # move REC to beginning of list\n",
    "    REC_index = header_fields.index('REC')\n",
    "    header_fields.insert(0, header_fields.pop(REC_index))\n",
    "    \n",
    "    # add BATCH & ID columns\n",
    "    header_fields.insert(0, \"BATCH\")\n",
    "    header_fields.insert(0, \"ID\")\n",
    "\n",
    "# Print the list for good measure\n",
    "print(header_fields)\n",
    "print(\"There are {} columns in the data.\".format(len(header_fields)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data Files\n",
    "\n",
    "* Ok, now we are at the real meat of the program, the code that parses the data files and transforms it into a CSV file\n",
    "* Because we are opening files and looping it is difficult to split this program into separate Jupyter cells (which is nice for exposition).\n",
    "\n",
    "### The Logic of this program:\n",
    "\n",
    "* Open a CSV file for writing\n",
    "* Write the headers to the file\n",
    "* Loop over each data file\n",
    "    * Open the file\n",
    "    * Parse the first line (which contains a REC field)\n",
    "    * Create a dictionary for the row\n",
    "    * Populate the REC, ID, and BATCH fields\n",
    "    * Loop over every line in the file\n",
    "        * Parse the line\n",
    "        * If the line is REC\n",
    "            * Write the row to the CSV file\n",
    "            * Create a new row dictionary\n",
    "        * If the line is extra data\n",
    "            * Concatinate the new data to existing values\n",
    "        * Else \n",
    "            * Add the data to the row dictionary\n",
    "    * Write the last row to the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a data file for writing\n",
    "with open(data_file_name, \"w\") as csvfile:\n",
    "    \n",
    "    # Create a DictWriter instead of CSVWriter because it is better for this use case\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header_fields)\n",
    "    \n",
    "    # Write the headers to the file\n",
    "    writer.writeheader()\n",
    "    \n",
    "\n",
    "    # Loop over all of the files \n",
    "    for file in file_list:\n",
    "        \n",
    "        # Display which file is being processed\n",
    "        print(\"Processing file {}\".format(file))\n",
    "        \n",
    "        # Open the QuadraSTAR file for reading\n",
    "        with open(file, \"r\") as f:\n",
    "            \n",
    "            # get the id from the file name\n",
    "            file_name = file.stem\n",
    "        \n",
    "            # read the first line of the file \n",
    "            # which should be REC\n",
    "            first_line = f.readline()\n",
    "            \n",
    "            # split the line based on fixed field lengths\n",
    "            # Strings can be sliced\n",
    "            header, value = first_line[0:6], first_line[7:]\n",
    "\n",
    "            # handle the \"--REC--\" case by replacing dashes with nothing\n",
    "            # strip extra whitespace and newlines\n",
    "            header = header.replace(\"-\", \" \").strip()\n",
    "\n",
    "            # remove newlines from the values\n",
    "            value = value.strip()\n",
    "\n",
    "            # Create a dictionary \n",
    "            row_dictionary = { field:\"\" for field in header_fields}\n",
    "            \n",
    "            # Add columns for REC, ID, and BATCH\n",
    "            row_dictionary[header] = value\n",
    "            row_dictionary['ID'] = file_name + \"_\" + value\n",
    "            row_dictionary['BATCH'] = file_name\n",
    "                        \n",
    "            # loop over lines in file\n",
    "            for line in f:\n",
    "\n",
    "                 # split the line based on fix field lengths\n",
    "                header, value = line[0:6], line[7:]\n",
    "\n",
    "                # handle the \"--REC--\" case and strip newlines\n",
    "                header = header.replace(\"-\", \" \").strip()\n",
    "                \n",
    "                # remove newlines from the values\n",
    "                value = value.strip()\n",
    "\n",
    "                # when we get to a new record\n",
    "                if header == \"REC\": \n",
    "                    # save the old record to disk\n",
    "                    writer.writerow(row_dictionary)\n",
    "                    # create a new empty row dictionary\n",
    "                    row_dictionary = { field:\"\" for field in header_fields}\n",
    "                    # put this REC in the new row dictionary\n",
    "                    row_dictionary[header] = value\n",
    "                    # add ID\n",
    "                    row_dictionary['ID'] = file_name + \"_\" + value\n",
    "                    # add Batch\n",
    "                    row_dictionary['BATCH'] = file_name\n",
    "              \n",
    "                # Check to see if an entry already exists in the dictionary\n",
    "                elif (header in row_dictionary) and row_dictionary[header] and (value != row_dictionary[header]):\n",
    "                    \n",
    "                    # append new value, separated by semi-colon\n",
    "                    row_dictionary[header] = row_dictionary[header] + \"; \" + value\n",
    "                \n",
    "                # This isn't a new or existing record, populate the dictionary\n",
    "                else: \n",
    "                    \n",
    "                    # populate the row_dictionary with values \n",
    "                    row_dictionary[header] = value\n",
    "            \n",
    "            # Write the last record to the CSV file\n",
    "            writer.writerow(row_dictionary)\n",
    "\n",
    "# Display completed message   \n",
    "print(\"Transformation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
