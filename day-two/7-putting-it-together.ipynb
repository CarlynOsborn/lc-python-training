{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It Together\n",
    "\n",
    "This program demonstrates how to programmatically retrieve data from the [Library of Congress API](https://libraryofcongress.github.io/data-exploration/index.html) and load that data into [Pandas](https://pandas.pydata.org/) for processing and aalysis. This program uses the [Requests]() Python library for making HTTP requests for the JSON representations of the [Selected Digitized Books](https://www.loc.gov/collections/selected-digitized-books/) collection. Using the APIs pagination features, the program grabs the first 5 pages of the collection with 50 items each for a total of 250 items. The program saves these index JSON files to disk and then proceeds to iterate through each result set downloading the JSON representation of all 250 items (and saving them to disk). Finally, some data is extracted from the item level JSON and put into a Pandas Dataframe for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory for saving files\n",
    "DATA_DIR = \"json-data/\"\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Depth parameter\n",
    "PAGE_LIMIT = 5\n",
    "\n",
    "\n",
    "# HTTP Parameters\n",
    "BASE_URL = \"https://loc.gov\"\n",
    "ENDPOINT = \"/collections/selected-digitized-books\"\n",
    "FORMAT = \"json\"\n",
    "RESULTS_PER_PAGE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Collection Index\n",
    "\n",
    "* This section fetches JSON representation of the collection and saves it to disk\n",
    "* It also shows how to do pagination by looping through a range from 1 to PAGE_LIMIT\n",
    "* Each iteration builds a URL for page based on the parameters set above and the current `page_num`\n",
    "* Save each index file to disk in the DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://loc.gov/collections/selected-digitized-books/?fo=json&c=50&sp=1\n",
      "Fetching https://loc.gov/collections/selected-digitized-books/?fo=json&c=50&sp=2\n",
      "Fetching https://loc.gov/collections/selected-digitized-books/?fo=json&c=50&sp=3\n",
      "Fetching https://loc.gov/collections/selected-digitized-books/?fo=json&c=50&sp=4\n",
      "Fetching https://loc.gov/collections/selected-digitized-books/?fo=json&c=50&sp=5\n"
     ]
    }
   ],
   "source": [
    "# Fetch the first n index pages with a loop for each page\n",
    "for page_num in range(1,PAGE_LIMIT+1):\n",
    "    \n",
    "    # build a URL of the index page\n",
    "    URL = BASE_URL + ENDPOINT + \"/?fo={FORMAT}&c={RESULTS}&sp={PAGE}\".format(FORMAT=FORMAT,\n",
    "                                                                             RESULTS=RESULTS_PER_PAGE,\n",
    "                                                                             PAGE=page_num)\n",
    "    \n",
    "    # Fetch and parse the index JSON                                                                     \n",
    "    print(\"Fetching\", URL)\n",
    "    response = requests.get(URL)\n",
    "    collection_index = response.json()\n",
    "\n",
    "    # Save the json to disk\n",
    "    file_name = DATA_DIR + \"/index_\" + str(page_num) + \".json\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(collection_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Individual Items\n",
    "\n",
    "* Open the saved index files and extract just the results JSON\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('json-data/index_1.json'),\n",
       " PosixPath('json-data/index_2.json'),\n",
       " PosixPath('json-data/index_3.json'),\n",
       " PosixPath('json-data/index_4.json'),\n",
       " PosixPath('json-data/index_5.json')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a glob pattern to load the index files\n",
    "collection_indexes = list(Path(DATA_DIR).glob(\"index_*.json\"))\n",
    "collection_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use the `read_text()` function from Pathlib to very quickly suck the JSON data into memory\n",
    "* This is a shortcut around `with open(index,'r') as f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the files into the JSON parser\n",
    "results_pile = [json.loads(index.read_text())['results'] for index in collection_indexes]\n",
    "# Check the length\n",
    "len(results_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_pile[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This isn't actually what we need, we want one list not a list of lists\n",
    "* We can't actually use a list comprehension here because we are *extending* the list\n",
    "* A list comprehension would give us a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list to hold the result data\n",
    "results_pile = []\n",
    "# loop over each index file\n",
    "for file in collection_indexes:\n",
    "    # use pathlib to quickly read the file and parse as JSON\n",
    "    index = json.loads(file.read_text())\n",
    "    # append all results to the results_pile list\n",
    "    results_pile.extend(index['results'])\n",
    "        \n",
    "# Check the length to make sure everything is loaded\n",
    "len(results_pile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This loop iterates over every idem takes a URL, based on the item ID, and fetches the JSON representation\n",
    "* It saves the raw JSON to disk as a backup and also returns the JSON representation\n",
    "* Also sleeps for one second to prevent rate limiting with 250 items this loop should take 250 seconds or around 4 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching item  0\n",
      "Fetching item  50\n",
      "Fetching item  100\n",
      "Fetching item  150\n",
      "Fetching item  200\n"
     ]
    }
   ],
   "source": [
    "# A function for retreiving the item level JSON\n",
    "for counter, result in enumerate(results_pile):\n",
    "    \n",
    "    if counter % 50 == 0:\n",
    "        print(\"Fetching item \", counter)\n",
    "    \n",
    "    url = result['id']\n",
    "    # add JSON format request\n",
    "    url = url + \"?fo=json\"\n",
    "    # fetch the url\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # try parsing the response data and catch exceptions\n",
    "    try:\n",
    "        # parse json\n",
    "        item_json = response.json()\n",
    "    except:\n",
    "        # if we get an error display why and stop looping\n",
    "        print(response.status_code)\n",
    "        print(response.headers)\n",
    "        break\n",
    "        \n",
    "    # get the lccn so we can make a unique filename\n",
    "    lccn = item_json['item'][\"library_of_congress_control_number\"]\n",
    "    # generate a filename and write json to disk\n",
    "    filename = DATA_DIR + \"item_\" + lccn + \".json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(item_json, f)\n",
    "    \n",
    "    # Sleep for one second\n",
    "    sleep(1)\n",
    "    \n",
    "# Display a finished message\n",
    "print(\"Download Complete. Fetched {} items.\".format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Files\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_files = list(Path(DATA_DIR).glob(\"item_*.json\"))\n",
    "len(item_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_item(path):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pile = [open_item(path) for path in item_files]\n",
    "len(item_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_of_interest = [\n",
    "    \"library_of_congress_control_number\",\n",
    "    \"date\",\n",
    "    \"title\",\n",
    "    \"medium\",\n",
    "    \"created_published\",\n",
    "    \"id\",\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "def get_fields(item):\n",
    "    \n",
    "    return {key : item['item'][key] for key in keys_of_interest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([get_fields(item) for item in item_pile])\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_lengths = pd.to_numeric(data['medium'].str.get(0).str.split().str.get(0),\n",
    "                            errors='coerce')\n",
    "book_lengths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_lengths.hist(bins=100, figsize=(10,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
